{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Learning about Reinforcement Learning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edfernandez-NU/AI_Startup_Prototype/blob/master/Learning_about_Reinforcement_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-TactYvoTh0"
      },
      "source": [
        "# Learning about Reinforcement Learning #\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deepmind/educational/blob/master/colabs/rl_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZplVq7RWWab"
      },
      "source": [
        "> <p><small><small><b>Copyright 2020 DeepMind Technologies Limited.</b></p>\n",
        "> <p><small><small> Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at </p>\n",
        "> <p><small><small> <a href=\"https://www.apache.org/licenses/LICENSE-2.0\">https://www.apache.org/licenses/LICENSE-2.0</a> </p>\n",
        "> <p><small><small> Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. </p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sRU73pBWZt7"
      },
      "source": [
        "\n",
        "**Aim**\n",
        "\n",
        "Reinforcement learning has been successful in solving challenging problems, such as the games of Go and StarCraft. It has significant potential for many real-world problems, such as drug discovery, global warming, or even discovering new scientific theories in physics, mathematics, and other fundamental sciences.\n",
        "\n",
        "\n",
        "This tutorial will go through an example of applying Reinforcement Learning to solve a problem and familiarise you with basic concepts used to formalise it. We will load an environment and an agent, specify a reward function, and then train a neural network to perform the task!\n",
        "\n",
        "**Disclaimer**\n",
        "\n",
        "This code is intended for educational purposes, and in the name of readability for a non-technical audience does not always follow best practices for software engineering.\n",
        "\n",
        "\n",
        "**Links to resources**\n",
        "- [What is Colab?](https://colab.sandbox.google.com/notebooks/intro.ipynb) If you have never used Colab before, get started here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-OB-7Yc0Y5z"
      },
      "source": [
        "## Reinforcement Learning\n",
        "\n",
        "\n",
        "In Reinforcement Learning we want to train an **agent** to maximise the total **reward** it receives within a fixed duration of interacting with an **environment**. \n",
        "\n",
        "The following diagram illustrates the interaction between the agent and environment.\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement_learning/rl_loop_illustrated.png\" width=\"500\" />\n",
        "</center>\n",
        "\n",
        "In this tutorial we will focus on simple environments to familiarise you with the process of training an agent. The simplest environment, shown below, is CartPole. This environment consists of a pole attached to a cart via a hinge. The agent needs to move the cart to the left or to the right in order to prevent the pole from falling over - this can be learned in just a few minutes.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement_learning/42135683-dde5c6f0-7d13-11e8-90b1-8770df3e40cf.gif\" width=\"500\" />\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twunuwjY-3Fq"
      },
      "source": [
        "#Pre-requisites#\n",
        "\n",
        "Before we start, we'll have to set up a few things. This colab will involve running a number of *cells*, each containing some code. If you look at the cell below, and hover over the brackets to the top left, it should turn into a play sign, that can be used to start or stop running the code in the cell.\n",
        "\n",
        "Click the play button on the next three cells below to install the software packages, import Python modules, and implement some functions that we'll use under the hood. (You should see it change to a \"stop\" icon while it's running.)\n",
        "\n",
        "This should only take around 30 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ShseGCk5hJpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e2e63b-a5d0-42ca-c801-1a49aa596351"
      },
      "source": [
        "#@title Install software packages  {'form-width':'30%'}\n",
        "%reset -f\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg\n",
        "!pip install gym\n",
        "!pip install imageio\n",
        "!pip install PILLOW\n",
        "!pip install pyglet\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb,tf,envs]\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,781 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,366 kB]\n",
            "Ign:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [443 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,691 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [54.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [257 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,218 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,131 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [865 kB]\n",
            "Get:23 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [46.5 kB]\n",
            "Fetched 11.1 MB in 4s (3,145 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl xvfb\n",
            "0 upgraded, 2 newly installed, 0 to remove and 64 not upgraded.\n",
            "Need to get 1,280 kB of archives.\n",
            "After this operation, 7,682 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.7 [783 kB]\n",
            "Fetched 1,280 kB in 1s (1,496 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144793 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.7_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.7) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.6/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.18.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (7.0.0)\n",
            "Requirement already satisfied: PILLOW in /usr/local/lib/python3.6/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2dtiKXsjHsEy"
      },
      "source": [
        "#@title Import python libraries {'form-width':'30%'}\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import dm_env\n",
        "import reverb\n",
        "import sonnet as snt\n",
        "import tensorflow as tf\n",
        "\n",
        "from acme import environment_loop\n",
        "from acme.tf import networks\n",
        "from acme.adders import reverb as adders\n",
        "from acme.agents.tf import actors\n",
        "from acme.datasets import reverb as datasets\n",
        "from acme.wrappers import atari_wrapper, gym_wrapper\n",
        "from acme import specs\n",
        "from acme import wrappers\n",
        "from acme.agents.tf import dqn\n",
        "from acme.agents import agent\n",
        "from acme.tf import utils\n",
        "from acme.utils import loggers\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcdefaults()\n",
        "plt.xkcd()\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "l12McIDx6o4J"
      },
      "source": [
        "#@title Set up utilities {'form-width':'30%'}\n",
        "\n",
        "\n",
        "def step_agent_in_environment(env, agent=None, num_episodes=3):\n",
        "  \"\"\"Steps an agent in an enviroment.\"\"\"\n",
        "  frames = []\n",
        "  actions = []\n",
        "\n",
        "  for n in range(num_episodes):\n",
        "    timestep = env.reset()\n",
        "    while not timestep.last():\n",
        "      frames.append(env.render(mode='rgb_array'))\n",
        "      if callable(agent):\n",
        "        action = agent(timestep.observation)\n",
        "      else:\n",
        "        action = agent.select_action(timestep.observation)\n",
        "      actions.append(action)\n",
        "      timestep = env.step(action)\n",
        "\n",
        "  return frames, actions\n",
        "\n",
        "\n",
        "def show_video(frames):\n",
        "  \"\"\"Show video.\"\"\"\n",
        "  video_filename = 'imageio.mp4'\n",
        "  # Write video:\n",
        "  with imageio.get_writer(video_filename, fps=60) as video:\n",
        "    for frame in frames:\n",
        "      video.append_data(frame)\n",
        "  # Read video and show it:\n",
        "  video = open(video_filename, 'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = \"\"\"\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>\"\"\".format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "\n",
        "\n",
        "print('All set!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShUEnEl824Sw"
      },
      "source": [
        "## Environments\n",
        "\n",
        "Next, let's load the RL environment we want to use!\n",
        "\n",
        "Environments in RL represent the task or problem that we are trying to solve. There are many types of environments, such as computer games, simulated robotics settings, etc. Some of these can take several hours or days for an agent to learn! \n",
        "\n",
        "First pick [CartPole](https://github.com/openai/gym/wiki/CartPole-v0) from the drop down menu as it is the fastest environment to train on. Later you can choose another environment to play with!\n",
        "\n",
        "Running this cell will show you an image of what the environment looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oK2YSbz3qWWT"
      },
      "source": [
        "#@title Load an environment\n",
        "environment_name = 'CartPole'  #@param ['MountainCar', 'CartPole', 'Atari']\n",
        "\n",
        "if 'CartPole' in environment_name:\n",
        "  environment_train = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n",
        "  environment_train = wrappers.SinglePrecisionWrapper(environment_train)\n",
        "  environment = gym_wrapper.GymWrapper(gym.make('CartPole-v0'))\n",
        "  environment = wrappers.SinglePrecisionWrapper(environment)\n",
        "  # Just for visualisation / evaluation, we'll set different angle limits\n",
        "  environment.env.theta_threshold_radians = 10.0\n",
        "elif 'MountainCar' in environment_name:\n",
        "  environment_train = gym_wrapper.GymWrapper(gym.make('MountainCar-v0'))\n",
        "  environment_train = wrappers.SinglePrecisionWrapper(environment_train)\n",
        "  environment = environment_train\n",
        "elif 'Atari' in environment_name:\n",
        "  environment_train = gym_wrapper.GymAtariAdapter(gym.make('Pong-v0'))\n",
        "  environment_train = atari_wrapper.AtariWrapper(environment_train)\n",
        "  environment_train = wrappers.SinglePrecisionWrapper(environment_train)\n",
        "  environment = environment_train\n",
        "else:\n",
        "  raise ValueError('Unknown environment: {}.'.format(environment_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CuxPrqoIGX9"
      },
      "source": [
        "# Random Agent\n",
        "\n",
        "Before we train an agent, we will start with a *random agent*. The random agent does not take environment observations into account when choosing to take an action and just chooses actions randomly from the set of all possible actions. You can generate random actions using the code below and see the behaviour of the agent:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvfFL9lp7bB"
      },
      "source": [
        "action_space = environment.action_space\n",
        "\n",
        "def int_random_action(state):\n",
        "  # state is unused for random agent\n",
        "  return action_space.sample()\n",
        "\n",
        "\n",
        "output = environment.reset()\n",
        "print('random action:', int_random_action(None))\n",
        "print('random action:', int_random_action(None))\n",
        "print('random action:', int_random_action(None))\n",
        "print('random action:', int_random_action(None))\n",
        "\n",
        "frames, actions = step_agent_in_environment(\n",
        "    env=environment, agent=int_random_action, num_episodes=5)\n",
        "\n",
        "print('actions = {}'.format(actions))\n",
        "\n",
        "show_video(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp0coy4-Gd_R"
      },
      "source": [
        "# Custom Agent\n",
        "\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement_learning/random_policy.png\" width=\"500\" />\n",
        "</center>\n",
        "\n",
        "Let's now try and manually code a better behaviour into the agent. The code below defines a way to run a custom set of actions. A value of 0 for the action means the cart is pushed left, and a value of 1 means the cart is pushed right.\n",
        "Currently, the code is set to a constant zero, which pushes the cart left all the time (you'll see this if you run it and play the video!).\n",
        "\n",
        "You can modify the function to specify what action should be taken depending on the observation.\n",
        "One thing you could try is to use an ***if statement***, which in Python, looks like the following.\n",
        "\n",
        "```\n",
        "if [some_condition]:\n",
        "  action = [some_value]\n",
        "else:\n",
        "  action = [some_other_value]\n",
        "```\n",
        "Looking at the code below, you have access to the `cart_position`, `cart_velocity`, `pole_angle`, and `pole_velocity_at_tip`.\n",
        "Can you think of what conditions you might use to set the actions based on these?\n",
        "**Ask for help if you're not sure!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nt0rlL2GcdQ"
      },
      "source": [
        "def custom_action_for_cartpole(state):\n",
        "  # for cartpole only:\n",
        "  cart_position = state[0]\n",
        "  cart_velocity = state[1]\n",
        "  pole_angle = state[2]\n",
        "  pole_velocity_at_tip = state[3]\n",
        "\n",
        "  # Instead of making the action 0 (in cartpole: go left), try to come up with\n",
        "  # a better behavior.\n",
        "  action = 0\n",
        "\n",
        "  return action\n",
        "\n",
        "\n",
        "output = environment.reset()\n",
        "\n",
        "frames, actions = step_agent_in_environment(\n",
        "    env=environment, agent=custom_action_for_cartpole, num_episodes=5)\n",
        "\n",
        "show_video(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUmtMIDD_Ly0"
      },
      "source": [
        "# How to train your agent? #\n",
        "\n",
        "Getting an agent to solve a task by using random actions is clearly not optimal. \n",
        "\n",
        "There are a lot of solutions invented by scientists around the world to solve the reinforcement learning problem. They all have different strengths and weaknesses. One of the most famous modern methods is called **Deep Q Networks (DQN)**, proposed by scientists at DeepMind in 2014, and was the first RL method to solve Atari games from images. \n",
        "\n",
        "Here we will use this method to train our agent. You don't need to worry about all the details but if you are interested you can later look at how changing some aspects of the code makes your agent learn faster or slower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgDj8IbsllUr"
      },
      "source": [
        "## Set up the agent #\n",
        "\n",
        "Hit run in the next cell to set up the agent. This will set up a *Q-network*, an *optimizer* that trains the network, a *replay buffer* which stores all of the experience obtained by the agent, and a DQN agent that puts these pieces together.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "W6UYjOigUCXb"
      },
      "source": [
        "#@title Agent setup  {'form-width':'30%'}\n",
        "\n",
        "def setup_agent(\n",
        "    environment,\n",
        "    learning_rate,\n",
        "    batch_size=64,\n",
        "    max_replay_size=1000,\n",
        "    logger=None,\n",
        "):\n",
        "  \"\"\"Setup the agent before training\"\"\"\n",
        "\n",
        "  environment_spec = specs.make_environment_spec(environment)\n",
        "\n",
        "  network = snt.Sequential([\n",
        "      lambda x: tf.cast(x, tf.float32),\n",
        "      snt.Flatten(),\n",
        "      snt.nets.MLP([100, environment_spec.actions.num_values])\n",
        "  ])\n",
        "\n",
        "  # Construct the agent.\n",
        "  agent = dqn.DQN(\n",
        "      environment_spec=environment_spec,\n",
        "      learning_rate=learning_rate,\n",
        "      batch_size=batch_size,\n",
        "      max_replay_size=max_replay_size,\n",
        "      network=network,\n",
        "      checkpoint=False,\n",
        "      logger=logger,\n",
        "  )\n",
        "\n",
        "  return agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWlUwmLVHFwM"
      },
      "source": [
        "## How to evaluate success?#\n",
        "<center>\n",
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement_learning/rl_question.png\" width=\"200\" />\n",
        "</center>\n",
        "\n",
        "\n",
        "Before we start training, how can we tell if our agents are any good? To answer that, we need some measure of 'goodness', which is usually related to the total rewards obtained over an episode, which is called the **return**. We can also talk about **loss** instead of reward: these are effectively just the opposite of one another. So we want to maximise the reward or minimise the loss.\n",
        "\n",
        "For example in CartPole the agent gets a positive reward of $1$ per timestep. The episode ends if the agent drops the pole or the cart is far from the centre. So in order to get as much reward as possible, the agent just needs to make the episode last as long as possible!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa8hnTpCqQ33"
      },
      "source": [
        "## Set up training#\n",
        "\n",
        "Hit run in the next cell to set up the training loop. This will set up code to take the agent we set up in the previous step, perform several thousands of training steps, and print the performance at each step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6_QyRgqbTTkd"
      },
      "source": [
        "#@title Training loop  {'form-width':'30%'}\n",
        "\n",
        "def train(environment, agent, num_training_episodes, log_every=10):\n",
        "  \"\"\"Train the agent via the DQN algorithm\"\"\"\n",
        "\n",
        "  min_actor_steps_before_learning = 1000\n",
        "  num_actor_steps_per_iteration = 1\n",
        "  num_learner_steps_per_iteration = 1\n",
        "  all_returns = []\n",
        "\n",
        "  learner_steps_taken = 0\n",
        "  actor_steps_taken = 0\n",
        "  for episode in range(num_training_episodes):\n",
        "\n",
        "    timestep = environment.reset()\n",
        "    agent.observe_first(timestep)\n",
        "    episode_return = 0\n",
        "\n",
        "    while not timestep.last():\n",
        "      # Get an action from the agent and step in the environment.\n",
        "      action = agent.select_action(timestep.observation)\n",
        "      next_timestep = environment.step(action)\n",
        "\n",
        "      # Record the transition.\n",
        "      agent.observe(action=action, next_timestep=next_timestep)\n",
        "\n",
        "      # Book-keeping.\n",
        "      episode_return += next_timestep.reward\n",
        "      actor_steps_taken += 1\n",
        "      timestep = next_timestep\n",
        "\n",
        "      # See if we have some learning to do.\n",
        "      if (actor_steps_taken >= min_actor_steps_before_learning and\n",
        "          actor_steps_taken % num_actor_steps_per_iteration == 0):\n",
        "        # Learn.\n",
        "        for learner_step in range(num_learner_steps_per_iteration):\n",
        "          agent.update()\n",
        "        learner_steps_taken += num_learner_steps_per_iteration\n",
        "\n",
        "    # Log quantities.\n",
        "    if episode % log_every == 0 or episode == num_training_episodes - 1:\n",
        "      print(f'Episode: {episode} | Return: {episode_return} | '\n",
        "            f'Learner steps: {learner_steps_taken} | '\n",
        "            f'Actor steps: {actor_steps_taken}')\n",
        "    all_returns.append(episode_return)\n",
        "\n",
        "  return all_returns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zdGnlMJfojH"
      },
      "source": [
        "## Train the agent!\n",
        "\n",
        "We need to specify the `hyperparameters` of our experiment to start the training. These refer to different parameters, like how many training steps (how long) we'll train the agent for, how fast it learns, how much memory it keeps to store previous experiences, etc.\n",
        "\n",
        "**In your first try, leave them at the default values for now** (num_training_steps = 200 and learning_rate = 3e-4) and hit run. Afterwards, come back and change the hyperparameters to make your agent learn faster and better!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AhJTddVDpOoB"
      },
      "source": [
        "#@title Train the agent, using some specific hyperparameters\n",
        "\n",
        "num_training_episodes = 200  # @param {type:\"integer\"}\n",
        "learning_rate = 3e-4  # @param {type:\"number\"}\n",
        "\n",
        "# Other parameters\n",
        "batch_size = 64\n",
        "max_replay_size = 100000\n",
        "\n",
        "# Set how often to print logs\n",
        "log_every = 10\n",
        "\n",
        "# Setup the agent\n",
        "\n",
        "class NoOpLogger(object):\n",
        "  \"\"\"Avoids logginng from Acme \"\"\"\n",
        "\n",
        "  def write(self, data):\n",
        "    pass\n",
        "\n",
        "agent_logger = NoOpLogger()\n",
        "\n",
        "agent = setup_agent(\n",
        "    environment_train,\n",
        "    learning_rate,\n",
        "    batch_size=batch_size,\n",
        "    max_replay_size=max_replay_size,\n",
        "    logger=agent_logger)\n",
        "\n",
        "# Use the training environment to train the agent\n",
        "returns = train(environment_train, agent, num_training_episodes, log_every)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWoh4-CypOoH"
      },
      "source": [
        "## Evaluate the agent\n",
        "Now that we have a trained agent, we need to *evaluate* its performance. We can do this in two ways:\n",
        "\n",
        " (1) quantitatively: plotting the *training curve*, which shows the performance of the agent (in terms of the total reward obtained) as a function of training time; and\n",
        "\n",
        " (2) qualitatively: using the trained agent to interact with the environment and generate a video.\n",
        "\n",
        "Run the two cells below to see each of these analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "2SYWH-qypOoH"
      },
      "source": [
        "#@title Plot the training curve {'form-width':'30%'}\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(0, num_training_episodes), returns)\n",
        "plt.grid(True)\n",
        "plt.xlabel('Episodes', fontsize=15)\n",
        "plt.ylabel('Total reward', fontsize=15)\n",
        "plt.tick_params(labelsize=15)\n",
        "plt.locator_params(nbins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cCRXVfUVgtHn"
      },
      "source": [
        "#@title Show video of the trained agent's behaviour {'form-width':'30%'}\n",
        "\n",
        "frames, actions = step_agent_in_environment(\n",
        "    env=environment, agent=agent, num_episodes=5)\n",
        "\n",
        "show_video(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDRMnnK6JZBW"
      },
      "source": [
        "## Analysis and post-mortem (what went wrong?)\n",
        "\n",
        "In this environment, by the end of training the agent should be able to reach a maximum reward of $200$ in an episode. Based on the plot above, and the printouts during training, did it get there?\n",
        "\n",
        "If not, perhaps we can try training again with some different hyperparameters?\n",
        "Try going back and increasing the number of training steps to $400$, which means the agent will learn for longer.\n",
        "You could also (separately) try increasing the *learning rate* from 3e-4 (i.e. $3 \\times 10^{-4}$) to 1e-3 (i.e. $1 \\times 10^{-3}$): this quantity varies how much the agent changes on each training step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzeLFP-Q9JbP"
      },
      "source": [
        "## Additional activities ##\n",
        "Now that you've used reinforcement learning to train an agent to perform the CartPole task, try giving the task a go yourself and see how you do! You can try controlling CartPole via keyboard at this link: https://fluxml.ai/experiments/cartPole/\n",
        "\n",
        "If you have any spare time, go back and change the hyperparameters to make your agent learn faster! You can also choose a different environment (eg. Atari or MountainCar), and run everything again!\n",
        "\n",
        "Note that Atari might take quite a long time to train - see for yourself!\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}